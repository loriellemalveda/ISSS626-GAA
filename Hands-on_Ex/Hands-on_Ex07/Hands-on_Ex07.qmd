---
title: "Hands-on Exercise 7: Calibrating Hedonic Pricing Model for Private High-rise Properties with the GWR Method"
author: "Lorielle Malveda"
date: "October 2, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  freeze: true
  warning: false
---

# 1. OVERVIEW

**Geographically Weighted Regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

In this Hands-On exercise, we are going to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models using GWR methods.

We will use 2015 RESALE CONDOMINIUM PRICES as our **dependent variable**.

The **independent variables** are divided into either structural or locational.

# 2. THE DATA

Two data sets will be used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format –\> `MP14_SUBZONE_WEB_PL`

-   condo_resale_2015 in csv format --\> `condo_resale_2015.csv`

# 3. GETTING STARTED

Let's install the necessary R packages and launch these into the environment.

-   [**olsrr**](https://olsrr.rsquaredacademy.com/): R package for building OLS and performing diagnostics tests

-   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html)**:** R package for calibrating geographical weighted family of models

-   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html): R package for multivariate data visualisation and analysis

-   **sf:** Spatial data handling

-   **tidyverse**, including **readr**, **ggplot2,** and **dplyr:** Attribute data handling

-   **tmap**: choropleth mapping

The code chunks below installs and launches these R packages into R environment.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gt, gtsummary)
```

# 4. A SHORT NOTE ABOUT *GWmodel*

**The [GWmodel](https://www.jstatsoft.org/article/view/v063i17)** package provides a collection of localized spatial statistical methods, which are:

1.  GW summary statistics
2.  GW principal components analysis
3.  GW discriminant analysis and various forms of GW regression

Typically, the outputs or parameters of the GWmodel are visualized through mapping, serving as a valuable exploratory tool that can often guide or complement more traditional or advanced statistical analyses.

# 5. GEOSPATIAL DATA WRANGLING

## 5.1 Importing Geospatial Data

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz = st_read(dsn = "geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

The report indicates that the R object containing the imported MP14_SUBZONE_WEB_PL shapefile is named *mpsz* and is classified as a simple feature object with a geometry type of *multipolygon*. It is also important to highlight that the *mpsz* object lacks EPSG (spatial reference system) information.

## 5.2 Updating CRS Information

The code chunk below updates the newly imported `mpsz` with the correct ESPG code (i.e. 3414)

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

After transforming the projection metadata, let's verify the projection of the newly transformed `mpsz_svy21` by using `st_crs()` of the ***sf*** package.

```{r}
st_crs(mpsz_svy21)
```

EPSG is tagged as *3414* now.

Next, let'sl reveal the extent of `mpsz_svy21` by using the `st_bbox()` function of the ***sf*** package.

```{r}
st_bbox(mpsz_svy21) #view extent
```

# 6. ASPATIAL DATA WRANGLING

## 6.1 Importing the Aspatial Data

Let's use the `read_csv()` function of the ***readr*** package to import `condo_resale_2015` into R as a tibble data frame called `condo_resale`.

```{r}
condo_resale = read_csv("data/Condo_resale_2015.csv")
```

Next, it is important for us to examine if the data file has been imported correctly.

Let's use `glimpse()` to display the data structure.

```{r}
glimpse(condo_resale)
```

```{r}
head(condo_resale$LONGITUDE) #see the data in XCOORD column
```

```{r}
head(condo_resale$LATITUDE) #see the data in YCOORD column
```

Next, `summary()` of base R is used to display the summary statistics of `condo_resale` tibble data frame.

```{r}
summary(condo_resale)
```

## 6.2 Converting the Aspatial Dataframe Into An *sf* Object

Currently, the `condo_resale` tibble data frame is aspatial data. We will convert this to an ***sf*** object using the code chunk below.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

The `st_transform()` function from the ***sf*** package is used to convert the coordinates from the WGS84 coordinate reference system (CRS: 4326) to SVY21 (CRS: 3414). After the transformation, the `head()` function is applied to display the first few rows of the *condo_resale.sf* object.

```{r}
head(condo_resale.sf)
```

The output is in a point feature data frame.

# 7. EXPLORATORY DATA ANALYSIS

In the section, we will perform EDA using the statistical graphics functions of the **ggplot2** package.

## 7.1 EDA Using Statistical Graphics

We can plot the distribution of `SELLING_PRICE` by using the appropriate Exploratory Data Analysis (EDA) techniques as shown in the code chunk below.

```{r}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

The figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.

Statistically, a skewed distribution can be normalized by applying a log transformation. The code snippet below demonstrates how to create a new variable, `LOG_SELLING_PRICE`, by applying a log transformation to the `SELLING_PRICE` variable. This operation is performed using the `mutate()` function from the ***dplyr*** package.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Now, let's plot the `LOG_SELLING_PRICE` using the code chunk below.

```{r}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Now, the distribution is relatively less skewed after the transformation.

## 7.2 Multiple Histogram Plots Distribution of Variables

In this section, let's create small multiple histograms, which is also known as a **trellis plot**, using the `ggarrange()` function from the ***ggpubr*** package.

The code chunk below generates 12 histograms and then organizes them into a 3-column by 4-row layout using `ggarrange()` to form a small multiple plot.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

## 7.3 Drawing the Statistical Point Map

Lastly, let's reveal the geospatial distribution of condominium resale prices in Singapore. The map will be prepared by using the ***tmap*** package.

Let's use the interactive mode of ***tmap*** by using the code chunk below.

```{r}
tmap_mode("view")
```

We will then create an interactive point symbol map using the code chunk below.

```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

It is important to note that `tm_dots()` is used here instead of `tm_bubbles()`. Additionally, the `set.zoom.limits` argument within the `tm_view()` function is utilized to set the minimum and maximum zoom levels to 11 and 14, respectively.

Before proceeding to the next section, the code snippet provided below will be used to switch R's display to plot mode.

```{r}
tmap_mode("plot")
```

# 8. HEDONIC PRICING MODELLING IN R

In this section, we are going to build **hedonic pricing models** for condominium resale units in Singapore using the `lm()` of R base.

## 8.1 Simple Linear Regression Method

First, we will build a simple linear regression model by using `SELLING_PRICE` as the dependent variable and `AREA_SQM` as the independent variable.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

The `lm()` function returns an object of class "lm", or for multiple responses, of class `c("mlm", "lm")`. The functions `summary()` and `anova()` can be used to generate and display a summary and an analysis of variance table for the results. Various useful components of the `lm` output, such as coefficients, effects, fitted values, and residuals, can be extracted using the respective generic accessor functions.

```{r}
summary(condo.slr)
```

The output report reveals that the `SELLING_PRICE` can be explained by using the formula:

```         
 *y = -258121.1 + 14719x1*
```

The R-squared value of 0.4518 indicates that the simple regression model explains approximately 45% of the variance in resale prices. Since the p-value is significantly smaller than 0.0001, we reject the null hypothesis that the mean is a good estimator of `SELLING_PRICE`. This suggests that the simple linear regression model is a good predictor of `SELLING_PRICE`.

In the "Coefficients" section of the report, it is shown that the p-values for both the intercept and the estimate of `ARA_SQM` are smaller than 0.001. Consequently, we reject the null hypothesis that B0 and B1 are equal to 0, leading us to conclude that B0 and B1 are reliable parameter estimates.

To visualize the best fit line on a scatterplot, we can use the `lm()` function as a method within `ggplot`'s geometry, as demonstrated in the code chunk below.

```{r}
ggplot(data=condo_resale.sf,          
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +   
   geom_point() +   
   geom_smooth(method = lm)
```

The figure above reveals that there are a few statistical outliers with relatively high selling prices.

## 8.2 Multiple Linear Regression Method

### VISUALIZING THE RELATIONSHIPS OF THE INDEPENDENT VARIABLES

Before constructing a multiple regression model, it's important to check that the independent variables are not highly correlated with each other. If highly correlated variables are mistakenly included in the model, it can reduce its accuracy, a problem known as **multicollinearity** in statistics.

A correlation matrix is commonly used to visualize the relationships between independent variables. In addition to R's `pairs()` function, many packages offer ways to display a correlation matrix. In this section, we will use the ***corrplot*** package.

The code chunk below demonstrates how to create a scatterplot matrix to explore the relationships between the independent variables in the `condo_resale` data frame.

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",          
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Matrix reordering is very important for uncovering hidden structures and patterns within a matrix. In ***corrplot***, there are four reordering methods (set using the `order` parameter): "AOE", "FPC", "hclust", and "alphabet". In the previous code, the AOE order was applied, which arranges variables based on the angular order of eigenvectors, a method suggested by Michael Friendly.

From the scatterplot matrix, it is evident that `Freehold` is highly correlated with `LEASE_99YEAR`. Given this, it is more prudent to include only one of these variables in the subsequent model. Therefore, `LEASE_99YEAR` is excluded from the following model-building process.

## 8.3 Building a Hedonic Pricing Model Using the Multiple Linear Regression Method

The code chunk below uses `lm()` to calibrate the multiple linear regression model.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    
                + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +                 PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN +                    PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH +  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,data=condo_resale.sf) 
summary(condo.mlr)
```

## 8.4 Preparing Publication Quality Table: the *OLSRR* Method

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revise the model by removing those variables that are not statistically significant.

Now, we are ready to calibrate the revised model by using the code chunk below.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE +                     PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +                    PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK +                     
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP +                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf)

ols_regress(condo.mlr1)
```

## 8.5 Preparing Publication Quality Table: the *gtsummary* Method

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/index.html) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

With the `gtsummary` package, model statistics can be included in the report by either appending them to the report table by using `add_glance_table()` or adding as a table source note by using `add_glance_source_note()` as shown in the code chunk below.

```{r}
tbl_regression(condo.mlr1,                 
               intercept = TRUE) %>%  add_glance_source_note(     
                 label = list(sigma ~ "\U03C3"),     
                 include = c(r.squared, adj.r.squared,                  
                             AIC, statistic,                 
                             p.value, sigma))
```

For additional customization options, you can refer to the **"Tutorial: tbl_regression"** documentation, which provides detailed guidance on how to further modify and tailor the output of regression tables to suit your needs.

### CHECKING FOR MULTICOLLINEARITY

In this section, we introduce a fantastic R package specifically designed for performing ordinary least squares (OLS) regression: `olsrr`. This package offers a collection of highly useful methods for improving multiple linear regression models, including:

-   Comprehensive regression output
-   Residual diagnostics
-   Measures of influence
-   Heteroskedasticity tests
-   Collinearity diagnostics
-   Model fit assessment
-   Variable contribution assessment
-   Variable selection procedures

In the code chunk below, the `ols_vif_tol()` function from the ***olsrr*** package is used to check for signs of multicollinearity in the model.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF values of the independent variables are less than 5, we can safely conclude that there is no sign of multicollinearity among the independent variables.

### TEST FOR NON-LINEARITY

In multiple linear regression, it is important for us to test the assumption that linearity and additive of the relationship between dependent and independent variables.

Using the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of the ***olsrr*** package, let's perform the linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

The figure above shows that most of the data points are scattered around the 0 line, indicating that the relationships between the dependent variable and the independent variables are likely linear. Therefore, we can reasonably conclude that the assumption of linearity holds for this model.

### TEST FOR NORMALITY ASSUMPTION

Lastly, the code chunk below uses `ols_plot_resid_hist()` of the ***olsrr*** package to perform the normality assumption test.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) resembles a normal distribution.

For more formal statistical test methods, refer to the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of the ***olsrr*** package as shown in the code chunk below.

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence, we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

### TESTING FOR SPATIAL AUTOCORRELATION

The hedonic model we are building incorporates geographically referenced attributes, making it important to visualize the residuals of the hedonic pricing model.

To conduct a spatial autocorrelation test, we need to convert the `condo_resale.sf` object from an `sf` data frame to a `SpatialPointsDataFrame`.

The first step is to extract the residuals from the hedonic pricing model and save them as a separate data frame.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

Next, let's join the newly created data frame with `condo_resale.sf` object.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, we will convert `condo_resale.res.sf` from a simple feature object into a `SpatialPointsDataFrame`, as the ***spdep*** package can only work with spatial data objects in the ***sp*** format.

The following code chunk demonstrates the data conversion process.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Next, we will use the ***tmap*** package to display the distribution of the residuals on an interactive map.

The code chunk below is used to create an interactive point symbol map.

```{r}
tmap_mode("view") 
tm_shape(mpsz_svy21)+   
  tmap_options(check.and.fix = TRUE) +   
  tm_polygons(alpha = 0.4) + 
  tm_shape(condo_resale.res.sf) +     
  tm_dots(col = "MLR_RES",           alpha = 0.6,           style="quantile") +   
  tm_view(set.zoom.limits = c(11,14))
```

Remember to switch back to “plot” mode before continuing!

```{r}
tmap_mode("plot")
```

The figure above indicates signs of spatial autocorrelation. To confirm this observation, we will perform the Moran’s I test.

First, we will compute the distance-based weight matrix using the `dnearneigh()` function from the *spdep* package.

```{r}
 nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE) 
 summary(nb)
```

Next, let's use the `nb2listw()` of the ***spdep*** package to convert the output neighbors lists (i.e. ***nb***) into spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W') 
summary(nb_lw)
```

After this, the `lm.morantest()` function from the ***spdep*** package will be used to perform Moran’s I test to assess residual spatial autocorrelation. This will help us determine whether the spatial distribution of residuals indicates significant spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran’s I test for residual spatial autocorrelation indicates a p-value of less than 0.00000000000000022, which is significantly smaller than the alpha value of 0.05. Therefore, we reject the null hypothesis that the residuals are randomly distributed.

Additionally, since the observed Global Moran’s I value is 0.1424418, which is greater than 0, we can infer that the residuals exhibit a clustered distribution pattern.

# 9. BUILDING HEDONIC PRICING MODELS USING *GWmodel*

In this section, we are going to model **hedonic pricing** using both fixed and adaptive bandwidth schemes.

## 9.1 Building Fixed Bandwidth GWR Model

### COMPUTING FIXED BANDWIDTH

In the code chunk below, the `bw.gwr()` function from the *GWModel* package is used to determine the optimal fixed bandwidth for the model. Note that the argument `adaptive` is set to `FALSE`, indicating that we are calculating a fixed bandwidth.

There are two possible approaches to determine the stopping rule: the cross-validation (CV) approach and the AIC corrected (AICc) approach. In this case, we define the stopping rule based on the agreement between these approaches.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +                       PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +                       PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +                      
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS +                       FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach="CV",                     kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

The result shows that the recommended bandwidth is 971.3405 meters. The result is in meters because the coordinate reference system (CRS) used in the model is likely a projected coordinate system, such as SVY21 (EPSG: 3414) or UTM, which measures distances in meters. Unlike geographic coordinate systems like WGS84, which use degrees to represent latitude and longitude, projected systems are designed for spatial analysis and use linear units like meters to provide accurate distance measurements over a specific area.

### GWModel METHOD - FIXED BANDWIDTH

Now we can use the code chunk below to calibrate the ***gwr*** model using the fixed bandwidth and gaussian kernel.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +                           PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +                           PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +                           PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS +                           FAMILY_FRIENDLY + FREEHOLD,                         
                       data=condo_resale.sp,                       
                       bw=bw.fixed,                     
                       kernel = 'gaussian',                    
                       longlat = FALSE)
```

The output is saved as a list of class "gwrm". The code below can be used to display the model output.

```{r}
gwr.fixed
```

The report reveals that the AICc (Akaike Information Criterion corrected) of the geographically weighted regression (GWR) model is 42263.61, which is significantly smaller than the AICc of the global multiple linear regression model, which is 42967.1. This suggests that the GWR model provides a better fit for the data compared to the global model.

## 9.2 Building Adaptive Bandwidth GWR Model

In this section, we will calibrate the gwr-based hedonic pricing model by using the adaptive bandwidth approach.

### COMPUTING THE ADAPTIVE BANDWIDTH

As in the earlier section, we will first use `bw.gwr()` to determine the optimal number of data points to use for the adaptive bandwidth. The code used for this process is very similar to the one used for calculating the fixed bandwidth, except that the `adaptive` argument is now set to `TRUE`. This change ensures that the bandwidth adapts to the density of data points, allowing for more flexibility in areas with varying data density.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  +                          PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    +                PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +                          PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP +                          NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,                       
                      data=condo_resale.sp,                      
                      approach="CV",                        
                      kernel="gaussian",                     
                      adaptive=TRUE,                       
                      longlat=FALSE)
```

The result shows that the 30 is the recommended data points to be used.

### CONSTRUCTING THE ADAPTIVE BANDWIDTH GWRModel

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel.

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE +                              PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +                              PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +                              PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +                              NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,                            
                          data=condo_resale.sp, 
                          bw=bw.adaptive,                            
                          kernel = 'gaussian',                
                          adaptive=TRUE,                        
                          longlat = FALSE)
```

The code below displays the model output.

```{r}
gwr.adaptive
```

The report shows that the AICc adaptive distance-gwr is 41982.22, which is smaller than the AICc fixed distance-gwr of 42263.61.

## 9.3 Visualizing GWR Output

In addition to the regression residuals, the output feature class table includes fields for observed and predicted *y* values, condition number (`cond`), Local R², residuals, explanatory variable coefficients, and their standard errors:

-   **Condition Number**: This diagnostic evaluates local collinearity. Strong local collinearity can make results unstable. Results with condition numbers greater than 30 may be unreliable.

-   **Local R²**: These values range from 0.0 to 1.0 and show how well the local regression model fits the observed *y* values. Low values indicate poor model performance in that area. Mapping the Local R² values can highlight where the GWR model predicts well and where it struggles, potentially identifying missing variables in the model.

-   **Predicted**: These are the fitted *y* values estimated by GWR.

-   **Residuals**: Residuals are calculated by subtracting the fitted *y* values from the observed *y* values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot color map of standardized residuals can be created to visualize these values.

-   **Coefficient Standard Error**: This measures the reliability of each coefficient estimate. Smaller standard errors relative to the coefficient values indicate higher confidence in the estimates. Large standard errors may signal local collinearity issues.

All these values are stored in a `SpatialPointsDataFrame` or `SpatialPolygonsDataFrame` object, which is integrated with fit points, GWR coefficient estimates, observed and predicted *y* values, coefficient standard errors, and t-values. These data are stored in the "data" slot of an object called `SDF` in the output list.

## 9.4 Converting SDF into ***sf*** data.frame

To visualize the fields in ***SDF***, let's first convert it into an ***sf*** data.frame.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%   st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF) 
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

Next, let's use `glimpse()` to display the content of the `condo_resale.sf.adaptive` ***sf*** data frame.

```{r}
glimpse(condo_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

## 9.5 Visualizing Local R2

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view") 
tm_shape(mpsz_svy21)+   
  tm_polygons(alpha = 0.1) + 
  tm_shape(condo_resale.sf.adaptive) +     
  tm_dots(col = "Local_R2",           
          border.col = "gray60",           
          border.lwd = 1) +   
  tm_view(set.zoom.limits = c(11,14))
```

```{r}
tmap_mode('plot')
```

## 9.6 Visualizing Coefficient Estimates

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view") 
AREA_SQM_SE <- tm_shape(mpsz_svy21)+   
  tm_polygons(alpha = 0.1) + 
  tm_shape(condo_resale.sf.adaptive) +     
  tm_dots(col = "AREA_SQM_SE",           
          border.col = "gray60",           
          border.lwd = 1) +   
  tm_view(set.zoom.limits = c(11,14))  
AREA_SQM_TV <- tm_shape(mpsz_svy21)+   
  tm_polygons(alpha = 0.1) + 
  tm_shape(condo_resale.sf.adaptive) +     
  tm_dots(col = "AREA_SQM_TV",           
          border.col = "gray60",           
          border.lwd = 1) +   
  tm_view(set.zoom.limits = c(11,14))  
tmap_arrange(AREA_SQM_SE, AREA_SQM_TV,               
             asp=1, ncol=2,              
             sync = TRUE)
```

```{r}
tmap_mode('plot')
```

### BY URA PLANNING REGION

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+   
  tm_polygons()+ 
  tm_shape(condo_resale.sf.adaptive) +    
  tm_bubbles(col = "Local_R2",            
             size = 0.15,           
             border.col = "gray60",  
             border.lwd = 1)
```

# 10. REFERENCES

Gollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) “GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models”. *Journal of Statistical Software*, 63(17):1-50, http://www.jstatsoft.org/v63/i17/

Lu B, Harris P, Charlton M, Brunsdon C (2014) “The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models”. *Geo-spatial Information Science* 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453
